{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:49:16.039945400Z",
     "start_time": "2024-04-28T10:49:16.037914400Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_document = \"Tokenization is the process of splitting a document into individual words or tokens. POS tagging assigns grammatical tags to tokens. Stop words are common words that are often removed. Stemming reduces words to their root form. Lemmatization is similar to stemming but considers the meaning of words.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\vivobook pro\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.4.16-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ----------------------------- ---------- 30.7/42.0 kB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 30.7/42.0 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 253.9 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vivobook pro\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.1/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.1/1.5 MB 939.4 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.2/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.2/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.3/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.6/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading regex-2024.4.16-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.9 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/268.9 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 225.3/268.9 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/268.9 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.9/268.9 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 2.8 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2024.4.16 tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:49:42.191300600Z",
     "start_time": "2024-04-28T10:49:34.894593Z"
    }
   },
   "id": "75d2a754b0e88b3b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vivobook\n",
      "[nltk_data]     Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vivobook\n",
      "[nltk_data]     Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Vivobook\n",
      "[nltk_data]     Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vivobook Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      "Tokenization is the process of splitting a document into individual words or tokens. POS tagging assigns grammatical tags to tokens. Stop words are common words that are often removed. Stemming reduces words to their root form. Lemmatization is similar to stemming but considers the meaning of words.\n",
      "\n",
      "Tokenization:\n",
      "['Tokenization', 'is', 'the', 'process', 'of', 'splitting', 'a', 'document', 'into', 'individual', 'words', 'or', 'tokens', '.', 'POS', 'tagging', 'assigns', 'grammatical', 'tags', 'to', 'tokens', '.', 'Stop', 'words', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', '.', 'Stemming', 'reduces', 'words', 'to', 'their', 'root', 'form', '.', 'Lemmatization', 'is', 'similar', 'to', 'stemming', 'but', 'considers', 'the', 'meaning', 'of', 'words', '.']\n",
      "\n",
      "POS Tagging:\n",
      "[('Tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('splitting', 'VBG'), ('a', 'DT'), ('document', 'NN'), ('into', 'IN'), ('individual', 'JJ'), ('words', 'NNS'), ('or', 'CC'), ('tokens', 'NNS'), ('.', '.'), ('POS', 'NNP'), ('tagging', 'VBG'), ('assigns', 'RB'), ('grammatical', 'JJ'), ('tags', 'NNS'), ('to', 'TO'), ('tokens', 'NNS'), ('.', '.'), ('Stop', 'VB'), ('words', 'NNS'), ('are', 'VBP'), ('common', 'JJ'), ('words', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('often', 'RB'), ('removed', 'VBN'), ('.', '.'), ('Stemming', 'VBG'), ('reduces', 'NNS'), ('words', 'NNS'), ('to', 'TO'), ('their', 'PRP$'), ('root', 'NN'), ('form', 'NN'), ('.', '.'), ('Lemmatization', 'NNP'), ('is', 'VBZ'), ('similar', 'JJ'), ('to', 'TO'), ('stemming', 'VBG'), ('but', 'CC'), ('considers', 'VBZ'), ('the', 'DT'), ('meaning', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('.', '.')]\n",
      "\n",
      "Stop Words Removal:\n",
      "['Tokenization', 'process', 'splitting', 'document', 'individual', 'words', 'tokens', '.', 'POS', 'tagging', 'assigns', 'grammatical', 'tags', 'tokens', '.', 'Stop', 'words', 'common', 'words', 'often', 'removed', '.', 'Stemming', 'reduces', 'words', 'root', 'form', '.', 'Lemmatization', 'similar', 'stemming', 'considers', 'meaning', 'words', '.']\n",
      "\n",
      "Stemming:\n",
      "['token', 'process', 'split', 'document', 'individu', 'word', 'token', '.', 'po', 'tag', 'assign', 'grammat', 'tag', 'token', '.', 'stop', 'word', 'common', 'word', 'often', 'remov', '.', 'stem', 'reduc', 'word', 'root', 'form', '.', 'lemmat', 'similar', 'stem', 'consid', 'mean', 'word', '.']\n",
      "\n",
      "Lemmatization:\n",
      "['Tokenization', 'process', 'splitting', 'document', 'individual', 'word', 'token', '.', 'POS', 'tagging', 'assigns', 'grammatical', 'tag', 'token', '.', 'Stop', 'word', 'common', 'word', 'often', 'removed', '.', 'Stemming', 'reduces', 'word', 'root', 'form', '.', 'Lemmatization', 'similar', 'stemming', 'considers', 'meaning', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(sample_document)\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Print results\n",
    "print(\"Original Document:\")\n",
    "print(sample_document)\n",
    "print(\"\\nTokenization:\")\n",
    "print(tokens)\n",
    "print(\"\\nPOS Tagging:\")\n",
    "print(pos_tags)\n",
    "print(\"\\nStop Words Removal:\")\n",
    "print(filtered_tokens)\n",
    "print(\"\\nStemming:\")\n",
    "print(stemmed_tokens)\n",
    "print(\"\\nLemmatization:\")\n",
    "print(lemmatized_tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:50:03.200270500Z",
     "start_time": "2024-04-28T10:49:52.260176700Z"
    }
   },
   "id": "c140931a291ad377"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assigns: 0.2182178902359924\n",
      "document: 0.2182178902359924\n",
      "grammatical: 0.2182178902359924\n",
      "individual: 0.2182178902359924\n",
      "into: 0.2182178902359924\n",
      "is: 0.2182178902359924\n",
      "of: 0.2182178902359924\n",
      "or: 0.2182178902359924\n",
      "pos: 0.2182178902359924\n",
      "process: 0.2182178902359924\n",
      "splitting: 0.2182178902359924\n",
      "tagging: 0.2182178902359924\n",
      "tags: 0.2182178902359924\n",
      "the: 0.2182178902359924\n",
      "to: 0.2182178902359924\n",
      "tokenization: 0.2182178902359924\n",
      "tokens: 0.4364357804719848\n",
      "words: 0.2182178902359924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample document\n",
    "sample_document = \"Tokenization is the process of splitting a document into individual words or tokens. POS tagging assigns grammatical tags to tokens.\"\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer and transform the document into a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform([sample_document])\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the TF-IDF values for each term in the document\n",
    "tfidf_values = tfidf_matrix.toarray()[0]\n",
    "\n",
    "# Create a dictionary to store the TF-IDF values for each term\n",
    "tfidf_dict = {term: tfidf_values[i] for i, term in enumerate(feature_names)}\n",
    "\n",
    "# Print the TF-IDF values for each term\n",
    "for term, tfidf in tfidf_dict.items():\n",
    "    print(f\"{term}: {tfidf}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:50:55.275904100Z",
     "start_time": "2024-04-28T10:50:55.258510500Z"
    }
   },
   "id": "2f3473a6c26cd68a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "20826620528937ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
