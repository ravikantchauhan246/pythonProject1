{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:49:16.039945400Z",
     "start_time": "2024-04-28T10:49:16.037914400Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_document = \"Tokenization is the process of splitting a document into individual words or tokens. POS tagging assigns grammatical tags to tokens. Stop words are common words that are often removed. Stemming reduces words to their root form. Lemmatization is similar to stemming but considers the meaning of words.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\vivobook pro\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.4.16-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ----------------------------- ---------- 30.7/42.0 kB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 30.7/42.0 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 253.9 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vivobook pro\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.1/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.1/1.5 MB 939.4 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.2/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.2/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.3/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.6/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading regex-2024.4.16-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.9 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/268.9 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 225.3/268.9 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/268.9 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.9/268.9 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 2.8 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2024.4.16 tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:49:42.191300600Z",
     "start_time": "2024-04-28T10:49:34.894593Z"
    }
   },
   "id": "75d2a754b0e88b3b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vivobook\n",
      "[nltk_data]     Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vivobook\n",
      "[nltk_data]     Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Vivobook\n",
      "[nltk_data]     Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vivobook Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      "Tokenization is the process of splitting a document into individual words or tokens. POS tagging assigns grammatical tags to tokens. Stop words are common words that are often removed. Stemming reduces words to their root form. Lemmatization is similar to stemming but considers the meaning of words.\n",
      "\n",
      "Tokenization:\n",
      "['Tokenization', 'is', 'the', 'process', 'of', 'splitting', 'a', 'document', 'into', 'individual', 'words', 'or', 'tokens', '.', 'POS', 'tagging', 'assigns', 'grammatical', 'tags', 'to', 'tokens', '.', 'Stop', 'words', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', '.', 'Stemming', 'reduces', 'words', 'to', 'their', 'root', 'form', '.', 'Lemmatization', 'is', 'similar', 'to', 'stemming', 'but', 'considers', 'the', 'meaning', 'of', 'words', '.']\n",
      "\n",
      "POS Tagging:\n",
      "[('Tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('splitting', 'VBG'), ('a', 'DT'), ('document', 'NN'), ('into', 'IN'), ('individual', 'JJ'), ('words', 'NNS'), ('or', 'CC'), ('tokens', 'NNS'), ('.', '.'), ('POS', 'NNP'), ('tagging', 'VBG'), ('assigns', 'RB'), ('grammatical', 'JJ'), ('tags', 'NNS'), ('to', 'TO'), ('tokens', 'NNS'), ('.', '.'), ('Stop', 'VB'), ('words', 'NNS'), ('are', 'VBP'), ('common', 'JJ'), ('words', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('often', 'RB'), ('removed', 'VBN'), ('.', '.'), ('Stemming', 'VBG'), ('reduces', 'NNS'), ('words', 'NNS'), ('to', 'TO'), ('their', 'PRP$'), ('root', 'NN'), ('form', 'NN'), ('.', '.'), ('Lemmatization', 'NNP'), ('is', 'VBZ'), ('similar', 'JJ'), ('to', 'TO'), ('stemming', 'VBG'), ('but', 'CC'), ('considers', 'VBZ'), ('the', 'DT'), ('meaning', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('.', '.')]\n",
      "\n",
      "Stop Words Removal:\n",
      "['Tokenization', 'process', 'splitting', 'document', 'individual', 'words', 'tokens', '.', 'POS', 'tagging', 'assigns', 'grammatical', 'tags', 'tokens', '.', 'Stop', 'words', 'common', 'words', 'often', 'removed', '.', 'Stemming', 'reduces', 'words', 'root', 'form', '.', 'Lemmatization', 'similar', 'stemming', 'considers', 'meaning', 'words', '.']\n",
      "\n",
      "Stemming:\n",
      "['token', 'process', 'split', 'document', 'individu', 'word', 'token', '.', 'po', 'tag', 'assign', 'grammat', 'tag', 'token', '.', 'stop', 'word', 'common', 'word', 'often', 'remov', '.', 'stem', 'reduc', 'word', 'root', 'form', '.', 'lemmat', 'similar', 'stem', 'consid', 'mean', 'word', '.']\n",
      "\n",
      "Lemmatization:\n",
      "['Tokenization', 'process', 'splitting', 'document', 'individual', 'word', 'token', '.', 'POS', 'tagging', 'assigns', 'grammatical', 'tag', 'token', '.', 'Stop', 'word', 'common', 'word', 'often', 'removed', '.', 'Stemming', 'reduces', 'word', 'root', 'form', '.', 'Lemmatization', 'similar', 'stemming', 'considers', 'meaning', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(sample_document)\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Print results\n",
    "print(\"Original Document:\")\n",
    "print(sample_document)\n",
    "print(\"\\nTokenization:\")\n",
    "print(tokens)\n",
    "print(\"\\nPOS Tagging:\")\n",
    "print(pos_tags)\n",
    "print(\"\\nStop Words Removal:\")\n",
    "print(filtered_tokens)\n",
    "print(\"\\nStemming:\")\n",
    "print(stemmed_tokens)\n",
    "print(\"\\nLemmatization:\")\n",
    "print(lemmatized_tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:50:03.200270500Z",
     "start_time": "2024-04-28T10:49:52.260176700Z"
    }
   },
   "id": "c140931a291ad377"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "docs = [ \"Sachin is considered to be one of the greatest cricket players\",\n",
    " \"Federer is considered one of the greatest tennis players\",\n",
    " \"Nadal is considered one of the greatest tennis players\",\n",
    " \"Virat is the captain of the Indian cricket team\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:14:29.918432600Z",
     "start_time": "2024-04-28T11:14:29.909593100Z"
    }
   },
   "id": "2f3473a6c26cd68a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sachin': 12, 'is': 7, 'considered': 2, 'to': 16, 'be': 0, 'one': 10, 'of': 9, 'the': 15, 'greatest': 5, 'cricket': 3, 'players': 11, 'federer': 4, 'tennis': 14, 'nadal': 8, 'virat': 17, 'captain': 1, 'indian': 6, 'team': 13}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer = \"word\", norm = None , use_idf = True , smooth_idf=True)\n",
    "Mat = vectorizer.fit(docs)\n",
    "print(Mat.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:14:37.639101200Z",
     "start_time": "2024-04-28T11:14:37.625092700Z"
    }
   },
   "id": "20826620528937ca"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tfidfMat = vectorizer.fit_transform(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:14:43.559086600Z",
     "start_time": "2024-04-28T11:14:43.552300800Z"
    }
   },
   "id": "2d6556d42ed77b8c"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t1.2231435513142097\n",
      "  (0, 3)\t1.5108256237659907\n",
      "  (0, 5)\t1.2231435513142097\n",
      "  (0, 15)\t1.0\n",
      "  (0, 9)\t1.0\n",
      "  (0, 10)\t1.2231435513142097\n",
      "  (0, 0)\t1.916290731874155\n",
      "  (0, 16)\t1.916290731874155\n",
      "  (0, 2)\t1.2231435513142097\n",
      "  (0, 7)\t1.0\n",
      "  (0, 12)\t1.916290731874155\n",
      "  (1, 14)\t1.5108256237659907\n",
      "  (1, 4)\t1.916290731874155\n",
      "  (1, 11)\t1.2231435513142097\n",
      "  (1, 5)\t1.2231435513142097\n",
      "  (1, 15)\t1.0\n",
      "  (1, 9)\t1.0\n",
      "  (1, 10)\t1.2231435513142097\n",
      "  (1, 2)\t1.2231435513142097\n",
      "  (1, 7)\t1.0\n",
      "  (2, 8)\t1.916290731874155\n",
      "  (2, 14)\t1.5108256237659907\n",
      "  (2, 11)\t1.2231435513142097\n",
      "  (2, 5)\t1.2231435513142097\n",
      "  (2, 15)\t1.0\n",
      "  (2, 9)\t1.0\n",
      "  (2, 10)\t1.2231435513142097\n",
      "  (2, 2)\t1.2231435513142097\n",
      "  (2, 7)\t1.0\n",
      "  (3, 13)\t1.916290731874155\n",
      "  (3, 6)\t1.916290731874155\n",
      "  (3, 1)\t1.916290731874155\n",
      "  (3, 17)\t1.916290731874155\n",
      "  (3, 3)\t1.5108256237659907\n",
      "  (3, 15)\t2.0\n",
      "  (3, 9)\t1.0\n",
      "  (3, 7)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidfMat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:14:49.127694500Z",
     "start_time": "2024-04-28T11:14:49.120654Z"
    }
   },
   "id": "6a1f80242d1bce61"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be' 'captain' 'considered' 'cricket' 'federer' 'greatest' 'indian' 'is'\n",
      " 'nadal' 'of' 'one' 'players' 'sachin' 'team' 'tennis' 'the' 'to' 'virat']\n"
     ]
    }
   ],
   "source": [
    "features_names = vectorizer.get_feature_names_out()\n",
    "print(features_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:14:56.451495200Z",
     "start_time": "2024-04-28T11:14:56.444500800Z"
    }
   },
   "id": "cd4d2d32b957d7ee"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dense = tfidfMat.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist , columns = features_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:15:04.877510200Z",
     "start_time": "2024-04-28T11:15:04.869387900Z"
    }
   },
   "id": "eab55204fdbe73e4"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "         be   captain  considered   cricket   federer  greatest    indian  \\\n0  1.916291  0.000000    1.223144  1.510826  0.000000  1.223144  0.000000   \n1  0.000000  0.000000    1.223144  0.000000  1.916291  1.223144  0.000000   \n2  0.000000  0.000000    1.223144  0.000000  0.000000  1.223144  0.000000   \n3  0.000000  1.916291    0.000000  1.510826  0.000000  0.000000  1.916291   \n\n    is     nadal   of       one   players    sachin      team    tennis  the  \\\n0  1.0  0.000000  1.0  1.223144  1.223144  1.916291  0.000000  0.000000  1.0   \n1  1.0  0.000000  1.0  1.223144  1.223144  0.000000  0.000000  1.510826  1.0   \n2  1.0  1.916291  1.0  1.223144  1.223144  0.000000  0.000000  1.510826  1.0   \n3  1.0  0.000000  1.0  0.000000  0.000000  0.000000  1.916291  0.000000  2.0   \n\n         to     virat  \n0  1.916291  0.000000  \n1  0.000000  0.000000  \n2  0.000000  0.000000  \n3  0.000000  1.916291  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>be</th>\n      <th>captain</th>\n      <th>considered</th>\n      <th>cricket</th>\n      <th>federer</th>\n      <th>greatest</th>\n      <th>indian</th>\n      <th>is</th>\n      <th>nadal</th>\n      <th>of</th>\n      <th>one</th>\n      <th>players</th>\n      <th>sachin</th>\n      <th>team</th>\n      <th>tennis</th>\n      <th>the</th>\n      <th>to</th>\n      <th>virat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.916291</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>1.510826</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.223144</td>\n      <td>1.223144</td>\n      <td>1.916291</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.916291</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>1.916291</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.223144</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.510826</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.916291</td>\n      <td>1.0</td>\n      <td>1.223144</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.510826</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>1.916291</td>\n      <td>0.000000</td>\n      <td>1.510826</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.916291</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.916291</td>\n      <td>0.000000</td>\n      <td>2.0</td>\n      <td>0.000000</td>\n      <td>1.916291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:15:15.815252700Z",
     "start_time": "2024-04-28T11:15:15.793434700Z"
    }
   },
   "id": "976aed789fd311ab"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             be   captain  considered   cricket   federer  greatest    indian  \\\n",
      "Doc 1  1.916291  0.000000    1.223144  1.510826  0.000000  1.223144  0.000000   \n",
      "Doc 2  0.000000  0.000000    1.223144  0.000000  1.916291  1.223144  0.000000   \n",
      "Doc 3  0.000000  0.000000    1.223144  0.000000  0.000000  1.223144  0.000000   \n",
      "Doc 4  0.000000  1.916291    0.000000  1.510826  0.000000  0.000000  1.916291   \n",
      "\n",
      "        is     nadal   of       one   players    sachin      team    tennis  \\\n",
      "Doc 1  1.0  0.000000  1.0  1.223144  1.223144  1.916291  0.000000  0.000000   \n",
      "Doc 2  1.0  0.000000  1.0  1.223144  1.223144  0.000000  0.000000  1.510826   \n",
      "Doc 3  1.0  1.916291  1.0  1.223144  1.223144  0.000000  0.000000  1.510826   \n",
      "Doc 4  1.0  0.000000  1.0  0.000000  0.000000  0.000000  1.916291  0.000000   \n",
      "\n",
      "       the        to     virat  \n",
      "Doc 1  1.0  1.916291  0.000000  \n",
      "Doc 2  1.0  0.000000  0.000000  \n",
      "Doc 3  1.0  0.000000  0.000000  \n",
      "Doc 4  2.0  0.000000  1.916291  \n"
     ]
    }
   ],
   "source": [
    "docList = ['Doc 1','Doc 2','Doc 3','Doc 4']\n",
    "skDocsIfIdfdf = pd.DataFrame(tfidfMat.todense(),index = sorted(docList), columns=features_names)\n",
    "print(skDocsIfIdfdf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:16:21.235580300Z",
     "start_time": "2024-04-28T11:16:21.225735700Z"
    }
   },
   "id": "f607dd8c7cdafd3"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "csim = cosine_similarity(tfidfMat,tfidfMat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:16:31.249223900Z",
     "start_time": "2024-04-28T11:16:31.239269600Z"
    }
   },
   "id": "d922756789fbf03f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "csimDf = pd.DataFrame(csim,index=sorted(docList),columns=sorted(docList))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:16:39.643378400Z",
     "start_time": "2024-04-28T11:16:39.635142300Z"
    }
   },
   "id": "abc843102bbe360b"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Doc 1     Doc 2     Doc 3     Doc 4\n",
      "Doc 1  1.000000  0.492416  0.492416  0.277687\n",
      "Doc 2  0.492416  1.000000  0.754190  0.215926\n",
      "Doc 3  0.492416  0.754190  1.000000  0.215926\n",
      "Doc 4  0.277687  0.215926  0.215926  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(csimDf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:16:45.457953900Z",
     "start_time": "2024-04-28T11:16:45.447641800Z"
    }
   },
   "id": "ee34799a51b4363f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "81e51166fa0828a9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
